{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here you can find the main imports to let the code run.\n",
    "If you don't want to run all of them at the same time, at the beginning of each step algorithm are listed the necessary imports for the specific execution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from step2.GreedyLearner import *\n",
    "from step3.Ucb import *\n",
    "from step3.Ucb import Ucb as Ucb3\n",
    "from step3.TS import *\n",
    "from step3.TS import TS as TS3\n",
    "from step4.UcbStep4 import *\n",
    "from step4.UcbStep4 import Ucb as Ucb4\n",
    "from step4.TSstep4 import *\n",
    "from step4.TSstep4 import TS as TS4\n",
    "from step5.UcbStep5 import *\n",
    "from step5.UcbStep5 import Ucb as Ucb5\n",
    "from step5.TSstep5 import *\n",
    "from step5.TSstep5 import TS as TS5\n",
    "from step6.Ucb_Sliding_window import *\n",
    "from step6.Ucb_Sliding_window import *\n",
    "from step7.ContextGenerator import ContextGenerator\n",
    "from step7.ContextNode import ContextNode\n",
    "from step7.ContextualLearner import ContextualLearner\n",
    "from step7.UcbStep7 import Ucb\n",
    "from step7.TSstep7 import TS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've developed a file called \"Settings.py\" where you can find all the relevant variables involved in the creation of the plots.\n",
    "* NUM_OF_DAYS: it represents the x-length of the plot, the number of days we use for learning the best arms\n",
    "* DAILY_INTERACTIONS: it represents the number of user interaction per day during which we do not update the arms but we just observe the rewards, increasing it lead to discover quicker the best arms\n",
    "* NUM_PLOT_ITERATION: it represents the number of time we restart the entire algorithm starting from 0, increasing it lead to consider more samples and so finding more reliable values\n",
    "\n",
    "We suggest to use different values in STEP 6 as the environment is no more static and so requires more data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "Settings.NUM_OF_DAYS = 100\n",
    "\n",
    "Settings.DAILY_INTERACTIONS = 20\n",
    "\n",
    "Settings.NUM_PLOT_ITERATION = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Greedy Learner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from step2.GreedyLearner import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "graph_sample = Graph(mode=\"full\", weights=True)\n",
    "env = EnvironmentPricing(4, graph_sample, 1)\n",
    "learner = Greedy_Learner(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                         graph_sample, env.alpha_ratios)\n",
    "learner.update()\n",
    "#print(learner.current_reward)\n",
    "print('\\nFINAL')\n",
    "print('Greedy algorithm chosen arms: ',learner.max_idxs)\n",
    "print('Clearvoyant best arms: [[0, 1, 2, 2, 3], [0, 2, 1, 0, 2], [1, 3, 1, 1, 1]]')\n",
    "print('Average reward with greedy algorithm choices: ', learner.revenue_given_arms(learner.max_idxs[0],0),\n",
    "      learner.revenue_given_arms(learner.max_idxs[1],1), learner.revenue_given_arms(learner.max_idxs[2],2))\n",
    "print('Average reward with best arms: ', learner.revenue_given_arms([0, 1, 2, 2, 3],0),\n",
    "      learner.revenue_given_arms([0, 2, 1, 0, 2],1), learner.revenue_given_arms([1, 3, 1, 1, 1],2))\n",
    "print('Average regret per iteration: ', learner.revenue_given_arms([0, 1, 2, 2, 3],0) - learner.revenue_given_arms(learner.max_idxs[0],0),\n",
    "      learner.revenue_given_arms([0, 2, 1, 0, 2], 1) - learner.revenue_given_arms(learner.max_idxs[1],1),\n",
    "      learner.revenue_given_arms([1, 3, 1, 1, 1], 2) - learner.revenue_given_arms(learner.max_idxs[2],2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 45,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [45]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m env \u001B[38;5;241m=\u001B[39m EnvironmentPricing(\u001B[38;5;241m4\u001B[39m, graph_sample, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      3\u001B[0m learner \u001B[38;5;241m=\u001B[39m Greedy_Learner(env\u001B[38;5;241m.\u001B[39mprices, env\u001B[38;5;241m.\u001B[39mconversion_rates, env\u001B[38;5;241m.\u001B[39mclasses, env\u001B[38;5;241m.\u001B[39msecondaries, env\u001B[38;5;241m.\u001B[39mnum_product_sold,\n\u001B[1;32m      4\u001B[0m                          graph_sample, env\u001B[38;5;241m.\u001B[39malpha_ratios)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mlearner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#print(learner.current_reward)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFINAL\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/onlinelearningapplications-2022/step2/GreedyLearner.py:205\u001B[0m, in \u001B[0;36mGreedy_Learner.update\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;66;03m# iterate until the local maximum has been found for each class\u001B[39;00m\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m local_max_found \u001B[38;5;241m==\u001B[39m [\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mTrue\u001B[39;00m]:\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mlearner\u001B[49m\u001B[38;5;241m.\u001B[39mmax_idxs)\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mprint\u001B[39m(learner\u001B[38;5;241m.\u001B[39mmax_revenue)\n\u001B[1;32m    208\u001B[0m     new_arms, new_revenue, chosen_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpull_arm()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'learner' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UCB-3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step3.Ucb import Ucb as Ucb3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    learner = Ucb3(4, env.prices, env.secondaries, env.num_product_sold[0], graph, env.alpha_ratios[0][1:])\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                              graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, a = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(arms=pulled_arms))\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "\n",
    "fig, ax = fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='UCB-1')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='UCB-1')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='UCB-1')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TS-3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step3.TS import TS as TS3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    learner = TS3(4, env.prices, env.secondaries, env.num_product_sold[0], graph,env.alpha_ratios[0][1:])\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                              graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, a = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(arms=pulled_arms))\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='TS-1')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='TS-1')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='TS-1')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UCB-4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step4.UcbStep4 import *\n",
    "from step4.UcbStep4 import Ucb as Ucb4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    learner = Ucb4(4, env.prices, env.secondaries, graph)\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                              graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(pulled_arms))\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    a = learner.revenue_given_arms(pulled_arms)\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "\n",
    "fig, ax = fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='UCB-4')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='UCB-4')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='UCB-4')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TS-4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step4.TSstep4 import *\n",
    "from step4.TSstep4 import TS as TS4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    learner = TS4(4, env.prices, env.secondaries, graph)\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                              graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    print(best_revenue)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(arms=pulled_arms))\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "print(learner.beta_parameters)\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='TS-4')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='TS-4')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='TS-4')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UCB-5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step5.UcbStep5 import *\n",
    "from step5.UcbStep5 import Ucb as Ucb5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    learner = Ucb5(4, env.prices, env.secondaries)\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                              graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(pulled_arms))\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='UCB-5')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='UCB-5')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='UCB-5')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TS-5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step5.TSstep5 import *\n",
    "from step5.TSstep5 import TS as TS5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    learner = TS5(4, env.prices, env.secondaries)\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold,\n",
    "                              graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(arms=pulled_arms))\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "\n",
    "fig, ax = fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='TS-5')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='TS-5')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='TS-5')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NUM_OF_DAYS = 700\n",
    "\n",
    "DAY_OF_ABRUPT_CHANGE = int(NUM_OF_DAYS/2)\n",
    "\n",
    "WINDOW_SIZE = int(np.sqrt(NUM_OF_DAYS))\n",
    "\n",
    "DAILY_INTERACTIONS = 200\n",
    "\n",
    "NUM_PLOT_ITERATION = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The environment is no more static, so we set new conversion rates that will substitute the previous run at a certain point of the simulation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "new_conv_rates=[\n",
    "    [\n",
    "      [0.7, 0.47, 0.45, 0.2],\n",
    "      [0.35, 0.3, 0.8, 0.25],\n",
    "      [0.45, 0.8, 0.4, 0.4],\n",
    "      [0.7, 0.35, 0.32, 0.25],\n",
    "      [0.5, 0.55, 0.85, 0.52] ],\n",
    "\n",
    "    [ [0.9, 0.45, 0.4, 0.35],\n",
    "      [0.4, 0.8, 0.3, 0.25],\n",
    "      [0.5, 0.45, 0.9, 0.35],\n",
    "      [0.4, 0.35, 0.8, 0.3],\n",
    "      [0.5, 0.45, 0.4, 0.9] ],\n",
    "    [\n",
    "      [0.25, 0.79, 0.4, 0.3],\n",
    "      [0.45, 0.4, 0.35, 0.95],\n",
    "      [0.55, 0.85, 0.5, 0.45],\n",
    "      [0.4, 0.82, 0.32, 0.25],\n",
    "      [0.4, 0.95, 0.35, 0.3]]\n",
    "  ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sliding Window"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step6.Ucb_Sliding_window import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = Non_stationary_environment(4, graph, 1)\n",
    "    learner = UCB_Sliding_Window(4, env.prices)\n",
    "\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold, graph, env.alpha_ratios)\n",
    "    clairvoyant_after_change = Clairvoyant(env.prices, new_conv_rates, env.classes, env.secondaries, env.num_product_sold, graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    best_revenue_after_change = clairvoyant_after_change.revenue_given_arms([0, 2, 1, 0, 2], 0)\n",
    "    best_revenue_array = [best_revenue for i in range(Settings.DAY_OF_ABRUPT_CHANGE)] + [best_revenue_after_change for i in range(Settings.DAY_OF_ABRUPT_CHANGE)]\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    best_rew = best_revenue\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        if i==Settings.DAY_OF_ABRUPT_CHANGE:\n",
    "            env.setNewConvRates(new_conv_rates)\n",
    "            print(\"Cambio\")\n",
    "            best_rew=best_revenue_after_change\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(arms=pulled_arms))\n",
    "        opt_rew.append(best_rew)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='UCB Sliding Window')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='UCB Sliding Window')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array)[:Settings.DAY_OF_ABRUPT_CHANGE], color='red', linestyle='--', label='Clairvoyant')\n",
    "xx = [i for i in range(Settings.DAY_OF_ABRUPT_CHANGE+1,Settings.NUM_OF_DAYS)]\n",
    "ax[1].plot(xx, np.cumsum(best_revenue_array)[Settings.DAY_OF_ABRUPT_CHANGE+1:],\n",
    "           color='green', linestyle='--', label='Clairvoyant after abrupt change')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='UCB Sliding Window')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, xmin=0., xmax=Settings.DAY_OF_ABRUPT_CHANGE/Settings.NUM_OF_DAYS,\n",
    "              color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].axhline(y=best_revenue_after_change, xmin=Settings.DAY_OF_ABRUPT_CHANGE/Settings.NUM_OF_DAYS, xmax=1.,\n",
    "              color='green', linestyle='--', label='Clairvoyant after abrupt change')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change Detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from step6.Ucb_Change_detection import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = Non_stationary_environment(4, graph, 1)\n",
    "    learner = Ucb_Change_detection(4, env.prices,env.secondaries)\n",
    "\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold, graph, env.alpha_ratios)\n",
    "    clairvoyant_after_change = Clairvoyant(env.prices, new_conv_rates, env.classes, env.secondaries, env.num_product_sold, graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms([0, 1, 2, 2, 3], 0)\n",
    "    best_revenue_after_change = clairvoyant_after_change.revenue_given_arms([0, 2, 1, 0, 2], 0)\n",
    "    best_revenue_array = [best_revenue for i in range(Settings.DAY_OF_ABRUPT_CHANGE)] + [best_revenue_after_change for i in range(Settings.DAY_OF_ABRUPT_CHANGE)]\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "    best_rew = best_revenue\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "        pulled_arms = learner.act()\n",
    "        print(pulled_arms)\n",
    "        if i==Settings.DAY_OF_ABRUPT_CHANGE:\n",
    "            env.setNewConvRates(new_conv_rates)\n",
    "            print(\"Cambio\")\n",
    "            best_rew=best_revenue_after_change\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        actual_rew.append(learner.revenue_given_arms(arms=pulled_arms))\n",
    "        print(actual_rew[-1])\n",
    "        opt_rew.append(best_rew)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k,:] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUM REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret= np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUM REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward= np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#AREWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward= np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(12,12))\n",
    "ax[0].plot(mean_cumulative_regret, color='blue', label='UCB Change Detection')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret,mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color='blue', label='UCB Change Detection')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array)[:Settings.DAY_OF_ABRUPT_CHANGE], color='red', linestyle='--', label='Clairvoyant')\n",
    "xx = [i for i in range(Settings.DAY_OF_ABRUPT_CHANGE+1,Settings.NUM_OF_DAYS)]\n",
    "ax[1].plot(xx, np.cumsum(best_revenue_array)[Settings.DAY_OF_ABRUPT_CHANGE+1:],\n",
    "           color='green', linestyle='--', label='Clairvoyant after abrupt change')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color='blue', label='UCB Change Detection')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, xmin=0., xmax=Settings.DAY_OF_ABRUPT_CHANGE/Settings.NUM_OF_DAYS,\n",
    "              color='red', linestyle='--', label='Clairvoyant')\n",
    "ax[2].axhline(y=best_revenue_after_change, xmin=Settings.DAY_OF_ABRUPT_CHANGE/Settings.NUM_OF_DAYS, xmax=1.,\n",
    "              color='green', linestyle='--', label='Clairvoyant after abrupt change')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 7"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Necessary imports for Step 7 to properly work are:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from step7.ContextGenerator import ContextGenerator\n",
    "from step7.ContextNode import ContextNode\n",
    "from step7.ContextualLearner import ContextualLearner\n",
    "from step7.UcbStep7 import Ucb\n",
    "from step7.TSstep7 import TS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Context Generation with Context-UCB1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We suggest the use of the following parameters to have a good-looking result:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Settings.NUM_PLOT_ITERATION = 20\n",
    "Settings.NUM_OF_DAYS = 90\n",
    "Settings.DAILY_INTERACTIONS = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, the code that regulates the execution is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color = 'blue'\n",
    "\n",
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    context_learner = ContextualLearner(features=env.features, n_arms=env.n_arms, n_products=len(env.graph.nodes))\n",
    "\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold, graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms(arms=[0, 1, 2, 2, 3], chosen_class=0)\n",
    "    best_revenue_array = [best_revenue for _ in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "    # optimal arm for C1: [0, 1, 2, 2, 3]\n",
    "    # optimal arm for C2: [0, 2, 1, 0, 2]\n",
    "    # optimal arm for C3: [1, 3, 1, 1, 1]\n",
    "    best_arms_per_class = [[0, 1, 2, 2, 3], [0, 2, 1, 0, 2], [1, 3, 1, 1, 1]]\n",
    "    best_disaggr_revenue = clairvoyant.disaggr_revenue_given_arms(arms=best_arms_per_class, env=env)\n",
    "\n",
    "    root_learner = Ucb(4, env.prices, env.secondaries, graph)\n",
    "\n",
    "    root_node = ContextNode(features=env.features, base_learner=root_learner)\n",
    "    context_learner.update_context_tree(root_node)\n",
    "\n",
    "    # confidence used for lower bounds is hardcoded to 0.1!\n",
    "    context_generator = ContextGenerator(features=env.features, contextual_learner=context_learner, confidence=0.1)\n",
    "\n",
    "    best_revenue = clairvoyant.disaggr_revenue_given_arms(arms=best_arms_per_class, env=env)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "\n",
    "        print(\"Execution \" + str(k+1) + \": day \" + str(i))\n",
    "\n",
    "        if i % 14 == 0 and i != 0:\n",
    "            context_generator.context_generation()\n",
    "\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "\n",
    "            customer = Customer(reservation_price=100, num_products=len(graph.nodes), graph=graph, env=env)\n",
    "\n",
    "            learner = context_learner.get_learner_by_context(current_features=customer.features)\n",
    "\n",
    "            pulled_arms = learner.act()\n",
    "\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms, customer)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "            context_generator.collect_daily_data(pulled_arms=pulled_arms,\n",
    "                                                 visited_products=visited_products,\n",
    "                                                 num_bought_products=num_bought_products,\n",
    "                                                 num_primaries=num_primary,\n",
    "                                                 features=customer.features)\n",
    "            customer.set_as_new()\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        context_generator.update_average_rewards(current_features=customer.features)\n",
    "\n",
    "        actual_rew.append(context_generator.average_rewards[-1])\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k, :] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUMULATIVE REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret = np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS*Settings.DAILY_INTERACTIONS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUMULATIVE REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward = np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#REWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward = np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\n",
    "ax[0].plot(mean_cumulative_regret, color=color, label='Ucb')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret, mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color=color, label='Ucb')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='grey', linestyle='--', label='Disaggregated Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color=color, label='Ucb')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='grey', linestyle='--', label='Disaggregated Clairvoyant')\n",
    "ax[2].axvline(x=14, color='red', label=\"Split attempt\")\n",
    "ax[2].axvline(x=28, color='red')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Context Generation with Context-TS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An appropriate choice of parameters in this case is the following:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Settings.NUM_PLOT_ITERATION = 20\n",
    "Settings.NUM_OF_DAYS = 90\n",
    "Settings.DAILY_INTERACTIONS = 220"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "and the execution code is the one below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color = 'green'\n",
    "\n",
    "final_reward= np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_regret = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "final_cumulative_reward = np.zeros((Settings.NUM_PLOT_ITERATION, Settings.NUM_OF_DAYS))\n",
    "\n",
    "for k in range (Settings.NUM_PLOT_ITERATION):\n",
    "\n",
    "    graph = Graph(mode=\"full\", weights=True)\n",
    "    env = EnvironmentPricing(4, graph, 1)\n",
    "    context_learner = ContextualLearner(features=env.features, n_arms=env.n_arms, n_products=len(env.graph.nodes))\n",
    "\n",
    "    clairvoyant = Clairvoyant(env.prices, env.conversion_rates, env.classes, env.secondaries, env.num_product_sold, graph, env.alpha_ratios)\n",
    "    best_revenue = clairvoyant.revenue_given_arms(arms=[0, 1, 2, 2, 3], chosen_class=0)\n",
    "    best_revenue_array = [best_revenue for _ in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "    # optimal arm for C1: [0, 1, 2, 2, 3]\n",
    "    # optimal arm for C2: [0, 2, 1, 0, 2]\n",
    "    # optimal arm for C3: [1, 3, 1, 1, 1]\n",
    "    best_arms_per_class = [[0, 1, 2, 2, 3], [0, 2, 1, 0, 2], [1, 3, 1, 1, 1]]\n",
    "    best_disaggr_revenue = clairvoyant.disaggr_revenue_given_arms(arms=best_arms_per_class, env=env)\n",
    "\n",
    "    root_learner = TS(4, env.prices, env.secondaries, graph)\n",
    "\n",
    "    root_node = ContextNode(features=env.features, base_learner=root_learner)\n",
    "    context_learner.update_context_tree(root_node)\n",
    "\n",
    "    # confidence used for lower bounds is hardcoded to 0.1!\n",
    "    context_generator = ContextGenerator(features=env.features, contextual_learner=context_learner, confidence=0.1)\n",
    "\n",
    "    best_revenue = clairvoyant.disaggr_revenue_given_arms(arms=best_arms_per_class, env=env)\n",
    "    opt_rew = []\n",
    "    actual_rew = []\n",
    "\n",
    "    for i in range(Settings.NUM_OF_DAYS):\n",
    "\n",
    "        print(\"Execution \" + str(k+1) + \": day \" + str(i))\n",
    "\n",
    "        if i % 14 == 0 and i != 0:\n",
    "            context_generator.context_generation()\n",
    "\n",
    "        for j in range(Settings.DAILY_INTERACTIONS):\n",
    "\n",
    "            customer = Customer(reservation_price=100, num_products=len(graph.nodes), graph=graph, env=env)\n",
    "\n",
    "            learner = context_learner.get_learner_by_context(current_features=customer.features)\n",
    "\n",
    "            pulled_arms = learner.act()\n",
    "\n",
    "            visited_products, num_bought_products, num_primary = env.round(pulled_arms, customer)\n",
    "            learner.updateHistory(pulled_arms, visited_products, num_bought_products, num_primary)\n",
    "            context_generator.collect_daily_data(pulled_arms=pulled_arms,\n",
    "                                                 visited_products=visited_products,\n",
    "                                                 num_bought_products=num_bought_products,\n",
    "                                                 num_primaries=num_primary,\n",
    "                                                 features=customer.features)\n",
    "            customer.set_as_new()\n",
    "\n",
    "        learner.update(pulled_arms)\n",
    "        context_generator.update_average_rewards(current_features=customer.features)\n",
    "\n",
    "        actual_rew.append(learner.average_reward[-1])\n",
    "        opt_rew.append(best_revenue)\n",
    "\n",
    "    final_cumulative_regret[k, :] = np.cumsum(opt_rew) - np.cumsum(actual_rew)\n",
    "    final_cumulative_reward[k, :] = np.cumsum(actual_rew)\n",
    "    final_reward[k:] = actual_rew\n",
    "\n",
    "\n",
    "#REGRET\n",
    "print(\"FINAL CUMULATIVE REGRET: \")\n",
    "print(final_cumulative_regret)\n",
    "\n",
    "mean_cumulative_regret = np.mean(final_cumulative_regret, axis=0)\n",
    "stdev_regret = np.std(final_cumulative_regret, axis=0) / np.sqrt(Settings.NUM_OF_DAYS*Settings.DAILY_INTERACTIONS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_regret)\n",
    "\n",
    "\n",
    "#Cumulative REWARD\n",
    "print(\"FINAL CUMULATIVE REWARD: \")\n",
    "print(final_cumulative_reward)\n",
    "\n",
    "mean_cumulative_reward = np.mean(final_cumulative_reward, axis=0)\n",
    "stdev_cumulative_reward = np.std(final_cumulative_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_cumulative_reward)\n",
    "\n",
    "#REWARD\n",
    "print(\"FINAL REWARD: \")\n",
    "print(final_reward)\n",
    "\n",
    "mean_final_reward = np.mean(final_reward, axis=0)\n",
    "stdev_reward = np.std(final_reward, axis=0) / np.sqrt(Settings.NUM_OF_DAYS)\n",
    "print(\"MEAN: \")\n",
    "print(mean_final_reward)\n",
    "\n",
    "best_revenue_array = [best_revenue for i in range(Settings.NUM_OF_DAYS)]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\n",
    "ax[0].plot(mean_cumulative_regret, color=color, label='TS')\n",
    "ax[0].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_regret - stdev_regret, mean_cumulative_regret + stdev_regret, alpha=0.4)\n",
    "ax[0].set_title('Cumulative Regret')\n",
    "\n",
    "ax[1].plot(mean_cumulative_reward, color=color, label='TS')\n",
    "ax[1].fill_between(range(Settings.NUM_OF_DAYS), mean_cumulative_reward - stdev_cumulative_reward, mean_cumulative_reward + stdev_cumulative_reward, alpha=0.4)\n",
    "ax[1].plot(np.cumsum(best_revenue_array), color='grey', linestyle='--', label='Disaggregated Clairvoyant')\n",
    "ax[1].set_title('Cumulative reward')\n",
    "\n",
    "ax[2].plot(mean_final_reward, color=color, label='TS')\n",
    "ax[2].fill_between(range(Settings.NUM_OF_DAYS), mean_final_reward - stdev_reward, mean_final_reward + stdev_reward, alpha=0.4)\n",
    "ax[2].axhline(y=best_revenue, color='grey', linestyle='--', label='Disaggregated Clairvoyant')\n",
    "ax[2].axvline(x=14, color='red', label=\"Split attempt\")\n",
    "ax[2].axvline(x=28, color='red')\n",
    "ax[2].set_title('Reward')\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}