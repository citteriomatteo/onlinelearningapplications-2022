import copy
import numpy as np

from step7.ContextNode import ContextNode
from step7.ContextualLearner import ContextualLearner


class ContextGenerator:
    """
    Class representing the Context generation algorithm: this object receives the data generated by the environment and
    determines how and when it is worth to generate a new Context.
    """

    def __init__(self, features: [], contextual_learner: ContextualLearner,
                 confidence: float):
        """
        Class constructor
        :param features: all the features considered.
        :param contextual_learner: learner that is capable of handling contexts. It is used an observer pattern, so the
        context generator keeps a reference of the overall learner.
        :param confidence: parameters used to determine the lower bound.
        """

        self.features = features
        self.collected_arms = np.array([], dtype=np.int)
        self.collected_visits = None
        self.collected_bought_products = None
        self.collected_features = None

        self.collected_next_purchases = np.array([], dtype=np.int)
        self.collected_past_pulled_arms = np.array([], dtype=np.int)
        self.collected_past_features = []

        self.contextual_learner = contextual_learner
        self.confidence = confidence

        self.context_tree = ContextNode(features, self.contextual_learner.get_root_learner())
        self.update_contextual_learner()

    def collect_daily_data(self, pulled_arms, visited_products, num_bought_products, features):
        """
        Collect the data produced by the environment in one day
        :param pulled_arms: arms that are pulled at day t
        :param visited_products: list of visited products at day t
        :param num_bought_products: list of bought products (in quantities) at day t
        :param features: features of the users that play an arm at day t
        """
        self.collected_arms = np.append(self.collected_arms, pulled_arms)

        if self.collected_visits is None:
            self.collected_visits = visited_products
        else:
            self.collected_visits = np.vstack((self.collected_visits, visited_products))

        if self.collected_bought_products is None:
            self.collected_bought_products = num_bought_products
        else:
            self.collected_bought_products = np.vstack((self.collected_bought_products, num_bought_products))

        if self.collected_features is None:
            self.collected_features = features
        else:
            self.collected_features = np.vstack((self.collected_features, features))

    def context_generation(self):
        """
        Algorithm that evaluates the possibility of generating a new context.
        """
        # get all the leaves that can be further expanded
        leaves = []
        for leaf in self.context_tree.get_leaves():
            if leaf.can_grow():
                leaves.append(leaf)
        if len(leaves) == 0:
            return
        # check if it is worth to split the leaves
        for leaf in leaves:
            self.evaluate_split(leaf)  # for each leaf evaluate if it is worth to split

    def evaluate_split(self, leaf: ContextNode):
        """
        Check is the leaf can be split.
        :param leaf: leaf to split.
        """

        features, values_after_split, right_learners, left_learners = self.iterate_over_features(leaf)
        # now get the max value after the split and the index
        max_value = max(values_after_split)
        idx = values_after_split.index(max_value)
        # check if the value is larger than the value before split
        before_learner = leaf.base_learner
        value_before = self.compute_lower_bound(before_learner.get_opt_arm_expected_value()[0],
                                                 len(before_learner.collected_rewards))

        if value_before < max_value:
            best_feature = features[idx]
            # there is a feature for which it is worth to split
            leaf.split(best_feature, left_learners[idx], right_learners[idx])
            self.update_contextual_learner()

    def iterate_over_features(self, leaf):
        """
        Check all the features that are not already expanded by the leaf and compute the related values after the split.
        The caller will check the maximum one and if it is greater than the value before split,
        the new context will be generated
        :param leaf: the leaf to be evaluated
        """
        values_after_split = []
        right_learners = []
        left_learners = []
        # get the features that are not expanded
        available_features = list(set(leaf.features) - set(leaf.features_subspace.keys()))
        for feature in available_features:
            # compute the probability that the split happens
            feature_id = self.features.index(feature)
            check_condition = [None for _ in self.features]
            # print(f'{check_condition=} - {len(check_condition)=}')
            for f in leaf.features_subspace:
                check_condition[self.features.index(f)] = leaf.features_subspace[f]
            # here all the indices of the values compliant with the current feature space
            indices = []
            left_split_indices = []
            right_split_indices = []
            # collect the indices of the samples that are compliant with the selected feature
            for idx, collected_feature in enumerate(self.collected_features):
                cond = True
                i = 0
                while cond and i < len(check_condition):
                    if check_condition[i] is None:
                        i += 1
                        continue
                    if check_condition[i] != collected_feature[i]:
                        cond = False
                    i += 1
                if cond:
                    indices.append(idx)
                    if not collected_feature[feature_id]:
                        left_split_indices.append(idx)
                    else:
                        right_split_indices.append(idx)

            # GREEDY ALGORITHM.
            left_split_probability = len(left_split_indices) / len(indices)
            right_split_probability = 1.0 - left_split_probability

            # get the left and right learners, trained with the data collected up to now
            left_subspace = copy.deepcopy(leaf.features_subspace)
            right_subspace = copy.deepcopy(leaf.features_subspace)
            left_subspace[feature] = False
            right_subspace[feature] = True
            left_learner = self.get_offline_trained_learner(pulled_arms=self.collected_arms[left_split_indices],
                                                            visits=self.collected_visits[left_split_indices],
                                                            num_bought=self.collected_bought_products[left_split_indices],
                                                            )
            right_learner = self.get_offline_trained_learner(pulled_arms=self.collected_arms[right_split_indices],
                                                             visits=self.collected_visits[right_split_indices],
                                                             num_bought=self.collected_bought_products[right_split_indices],
                                                             )
            left_value = left_learner.get_opt_arm_value()
            right_value = right_learner.get_opt_arm_value()
            value_after = self.compute_lower_bound(left_split_probability, len(left_split_indices)) * \
                          self.compute_lower_bound(left_value, len(left_learner.collected_rewards)) + \
                          self.compute_lower_bound(right_split_probability, len(right_split_indices)) * \
                          self.compute_lower_bound(right_value, len(right_learner.collected_rewards))

            values_after_split.append(value_after)
            right_learners.append(right_learner)
            left_learners.append(left_learner)
        return available_features, values_after_split, right_learners, left_learners

    def update_contextual_learner(self):
        """
        Method used to update the tree of the contextual learner.
        """
        self.contextual_learner.update_context_tree(self.context_tree)

    def compute_lower_bound(self, mean, n_samples):
        """
        Method used to compute the lower bound [Hoeffding-Bound]
        :param mean: mean of the distribution
        :param n_samples: cardinality
        """
        if n_samples == 0:
            return -np.inf
        ret_value = mean - np.sqrt(-np.log(self.confidence) / (2 * n_samples))
        return ret_value

    def get_offline_trained_learner(self, pulled_arms, visits, num_bought):
        """
        Train a new learner to be set as base learner of a new context
        :param pulled_arms: history of pulled arms
        :param rewards: history of rewards received by the environment
        :param costs: history of received costs
        :param subspace: feature subspace of the learner
        :return:
        """
        learner = self.contextual_learner.get_root_learner()

        for a, v, n in zip(pulled_arms, visits, num_bought):
            learner.updateHistory(a, v, n)
        learner.update(pulled_arms)
        return learner
